{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd5d3b5-ceba-469b-85e6-a6e0d1e3370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install numpy==1.26.4\n",
    "# !pip install openml dimod dwave-system\n",
    "#!pip install xlsxwriter\n",
    "#!pip install dwave-ocean-sdk\n",
    "#!pip install imblearn\n",
    "# Set your DWAVE_API_TOKEN\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e928b483-ef3b-4ccb-a0d4-626315e5f894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef, cohen_kappa_score\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import dimod\n",
    "from dwave.system import EmbeddingComposite, DWaveSampler, LeapHybridCQMSampler\n",
    "from dwave.samplers import SimulatedAnnealingSampler, TabuSampler, RandomSampler, PlanarGraphSolver, SteepestDescentSolver\n",
    "import xlsxwriter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "#os.environ['DWAVE_API_TOKEN'] = 'Actual-DW-key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f987a1f-6b55-4f4f-9f20-f3313ab66ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MC2.csv\n"
     ]
    }
   ],
   "source": [
    "def dataset(df, seed_value):\n",
    "    \"\"\"\n",
    "    Prepares a dataset by imputing missing values, handling imbalanced data, and normalizing features.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): Input dataframe where the last column is the target variable.\n",
    "    - seed_value (int): Random seed for reproducibility in data splitting and SMOTE.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_scaled (numpy.ndarray): Scaled training features.\n",
    "    - X_test_scaled (numpy.ndarray): Scaled testing features.\n",
    "    - y_train (numpy.ndarray): Training labels.\n",
    "    - y_test (numpy.ndarray): Testing labels.\n",
    "\n",
    "    Notes:\n",
    "    - Missing values in the features are imputed with the mean of each column.\n",
    "    - SMOTE is used to handle class imbalance by oversampling the minority class.\n",
    "    - The data is split into training and testing sets with an 80-20 split.\n",
    "    - Features are normalized using MinMaxScaler.\n",
    "    \"\"\"\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    # Impute missing values with the mean\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    X = X.astype(float)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    # Handle imbalanced data using SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=seed_value)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=seed_value)\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "#--------------------------------------------------------------------\n",
    "def feature_selection_mutual(X_train_scaled, X_test_scaled, y_train, solver_file='TS,default.csv'):\n",
    "    \"\"\"\n",
    "    Selects features based on mutual information using a specified solver.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train_scaled (numpy.ndarray): Scaled training features.\n",
    "    - X_test_scaled (numpy.ndarray): Scaled testing features.\n",
    "    - y_train (numpy.ndarray): Training labels.\n",
    "    - solver_file (str): File identifier for selecting the solver. Format is '<solver_name>,<filename>'.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_selected (numpy.ndarray): Scaled training features with selected features.\n",
    "    - X_test_selected (numpy.ndarray): Scaled testing features with selected features.\n",
    "    - features (numpy.ndarray): Indices of selected features.\n",
    "    - sampleset_info (dict): Information about the sampling process.\n",
    "\n",
    "    Notes:\n",
    "    - The function calculates mutual information between features and selects the most informative features using binary quadratic programming.\n",
    "    - Supported solvers include TabuSampler, SimulatedAnnealingSampler, PlanarGraphSolver, RandomSampler, SteepestDescentSolver, EmbeddingComposite, and LeapHybridCQMSampler.\n",
    "    \"\"\"\n",
    "    solver_name = solver_file.split(',')[0]\n",
    "    filename = solver_file.split(',')[1]\n",
    "    problem_label = f\"{solver_name}-{filename}\"\n",
    "    mutual_info_matrix_features = mutual_info_classif(X_train_scaled, y_train)\n",
    "\n",
    "    num_features = X_test_scaled.shape[1]\n",
    "    mutual_info_matrix_pairs = np.zeros((num_features, num_features))\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(num_features):\n",
    "            mutual_info_matrix_pairs[i, j] = mutual_info_classif(X_train_scaled[:, i].reshape(-1, 1), y_train)\n",
    "\n",
    "    alpha = 0.99\n",
    "    Rxy = mutual_info_matrix_features\n",
    "    Q = mutual_info_matrix_pairs * (1 - alpha)\n",
    "    np.fill_diagonal(Q, -Rxy * alpha)\n",
    "\n",
    "    bqm = dimod.BinaryQuadraticModel(Q, \"BINARY\")\n",
    "\n",
    "    if solver_name == 'TS':\n",
    "        solver = TabuSampler()\n",
    "    elif solver_name == 'SA':\n",
    "        solver = SimulatedAnnealingSampler()\n",
    "    elif solver_name == 'PG':\n",
    "        solver = PlanarGraphSolver()\n",
    "    elif solver_name == 'RS':\n",
    "        solver = RandomSampler()\n",
    "    elif solver_name == 'SD':\n",
    "        solver = SteepestDescentSolver()\n",
    "    elif solver_name == 'QPU':\n",
    "        solver = EmbeddingComposite(DWaveSampler())\n",
    "    elif solver_name == 'DW':\n",
    "        cqm = dimod.ConstrainedQuadraticModel()\n",
    "        cqm.set_objective(bqm)\n",
    "        solver = LeapHybridCQMSampler()\n",
    "\n",
    "    sampleset = solver.sample(bqm, num_reads=100, label=problem_label) if solver_name != 'DW' else solver.sample_cqm(cqm, label=problem_label)\n",
    "\n",
    "    best = sorted(sampleset.data(), key=lambda x: (list(x.sample.values())[0], x.energy))[0]\n",
    "    is_selected = np.array([bool(val) for val in best.sample.values()])\n",
    "    features = np.array([i for i, val in enumerate(is_selected) if val])\n",
    "\n",
    "    X_train_selected = X_train_scaled[:, features]\n",
    "    X_test_selected = X_test_scaled[:, features]\n",
    "\n",
    "    return X_train_selected, X_test_selected, features, sampleset.info\n",
    "#--------------------------------------------------------------------\n",
    "# Function to return full features\n",
    "def feature_full(X_train_scaled, X_test_scaled, y_train):\n",
    "    features = 'full'\n",
    "    info = 'X'\n",
    "    return X_train_scaled, X_test_scaled, features, info\n",
    "#--------------------------------------------------------------------\n",
    "def model_accuracy(X_train, X_test, y_train, y_test, seed_value):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of an SVC model on the provided dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (numpy.ndarray): Training features.\n",
    "    - X_test (numpy.ndarray): Testing features.\n",
    "    - y_train (numpy.ndarray): Training labels.\n",
    "    - y_test (numpy.ndarray): Testing labels.\n",
    "    - seed_value (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing various performance metrics:\n",
    "        - 'Accuracy': Classification accuracy.\n",
    "        - 'Precision': Weighted precision score.\n",
    "        - 'Recall': Weighted recall score.\n",
    "        - 'F1 Score': Weighted F1 score.\n",
    "        - 'AUC': Area Under the ROC Curve.\n",
    "        - 'G-mean': Geometric mean of sensitivity and specificity.\n",
    "        - 'Matthew': Matthews correlation coefficient.\n",
    "        - 'Cohen': Cohen's kappa score.\n",
    "        - 'Feature No': Number of features used.\n",
    "\n",
    "    Notes:\n",
    "    - The function uses a Support Vector Classifier (SVC) with probability estimates.\n",
    "    - Metrics include accuracy, precision, recall, F1 score, AUC, sensitivity, specificity, G-mean, MCC, and kappa.\n",
    "    \"\"\"\n",
    "    clf = SVC(probability=True, random_state=seed_value)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_prob = clf.predict_proba(X_test)\n",
    "    y_scores = clf.decision_function(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    auc = roc_auc_score(y_test, y_scores, multi_class='ovr')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    g_mean = (sensitivity * specificity) ** 0.5\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    num_columns = X_train.shape[1]\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'AUC': auc,\n",
    "        'G-mean': g_mean,\n",
    "        'Matthew': mcc,\n",
    "        'Cohen': kappa,\n",
    "        'Feature No': num_columns\n",
    "    }\n",
    "#--------------------------------------------------------------------\n",
    "def get_info(dictionary, key):\n",
    "    # Check if the key exists directly in the dictionary\n",
    "    if key in dictionary:\n",
    "        return dictionary[key]\n",
    "    \n",
    "    # Check if the 'timing' key exists and the key is in the nested 'timing' dictionary\n",
    "    if 'timing' in dictionary and key in dictionary['timing']:\n",
    "        return dictionary['timing'][key]\n",
    "    \n",
    "    # Return None if the key is not found in either case\n",
    "    return '---'\n",
    "#--------------------------------------------------------------------\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to preprocess datasets, apply feature selection methods, and evaluate models.\n",
    "    The results are saved to an Excel file.\n",
    "\n",
    "    Steps:\n",
    "    1. Set file paths and directories for datasets and results.\n",
    "    2. Iterate over dataset files in the selected directory.\n",
    "    3. Preprocess each dataset based on its type (NASA, JIRA, AEEEM).\n",
    "    4. Apply feature selection methods and evaluate models.\n",
    "    5. Save the results to an Excel file, with each dataset's results in a separate sheet.\n",
    "\n",
    "    Notes:\n",
    "    - Handles different preprocessing for NASA, JIRA, and AEEEM datasets.\n",
    "    - Uses `feature_full` and `feature_selection_mutual` for feature selection.\n",
    "    - Evaluates models with `model_accuracy`.\n",
    "    \"\"\"\n",
    "    num_runs = 1\n",
    "    #data_path = '/content/drive/MyDrive/Colab Notebooks/dwExp/'\n",
    "    data_path = 'dw_datasets/'\n",
    "    result_path = 'dw_results/'\n",
    "    folder_path = [data_path + 'NASA/', data_path + 'JIRA/', data_path + 'AEEEM/', data_path+'TEST/']\n",
    "    selected_path = folder_path[3]\n",
    "    data_dir_name = selected_path.split('/')[-2] #such as NASA, JIRA, etc. \n",
    "    \n",
    "    result_file_path = f'{result_path}{data_dir_name}_results.xlsx'\n",
    "\n",
    "    dataset_file_names = [file_name for file_name in os.listdir(selected_path) if file_name.endswith('.csv')]\n",
    "    results_dict = {}\n",
    "\n",
    "    for file_name in dataset_file_names:\n",
    "        print(file_name)\n",
    "        results_df = pd.DataFrame()\n",
    "        file_path = os.path.join(selected_path, file_name)\n",
    "        csv_data = pd.read_csv(file_path)\n",
    "\n",
    "        # Handle NASA dataset specific preprocessing\n",
    "        if 'NASA' in selected_path or 'TEST' in selected_path:\n",
    "            csv_data.replace('?', np.nan, inplace=True)\n",
    "            mapping = {'Y': 1, 'N': 0}\n",
    "            csv_data.iloc[:, -1] = csv_data.iloc[:, -1].map(mapping)\n",
    "            # Ensure that all values are correctly mapped\n",
    "            csv_data.iloc[:, -1] = pd.to_numeric(csv_data.iloc[:, -1], errors='coerce')\n",
    "\n",
    "        elif 'JIRA' in selected_path:\n",
    "            csv_data.replace('?', np.nan, inplace=True)\n",
    "            csv_data.iloc[:, -1] = csv_data.iloc[:, -1].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "        elif 'AEEEM' in selected_path:\n",
    "            csv_data.replace('?', np.nan, inplace=True)\n",
    "            mapping = {'buggy': 1, 'clean': 0}\n",
    "            csv_data.iloc[:, -1] = csv_data.iloc[:, -1].map(mapping)\n",
    "\n",
    "        # Drop columns with a single unique value or all missing values\n",
    "        csv_data = csv_data.loc[:, csv_data.nunique() != 1]\n",
    "        csv_data = csv_data.dropna(axis=1, how='all')\n",
    "\n",
    "        for run in range(num_runs):\n",
    "            seed_value = 41  # Set your desired seed value\n",
    "            X_train_scaled, X_test_scaled, y_train, y_test = dataset(csv_data, seed_value)\n",
    "\n",
    "            function_dict = {\n",
    "                'All_feature': (feature_full, (X_train_scaled, X_test_scaled, y_train)),\n",
    "                'approaches_DW_Hybrid_mutual': (feature_selection_mutual, (X_train_scaled, X_test_scaled, y_train, f'DW,{data_dir_name}_{file_name.split('.')[0]}')),\n",
    "                'approaches_DW_QPU_mutual': (feature_selection_mutual, (X_train_scaled, X_test_scaled, y_train, f'QPU,{data_dir_name}_{file_name.split('.')[0]}')),\n",
    "            }\n",
    "\n",
    "            for function_name, (function, args) in function_dict.items():\n",
    "                start_time = time.time()\n",
    "                X_train_selected, X_test_selected, features, info = function(*args)\n",
    "                metrics = model_accuracy(X_train_selected, X_test_selected, y_train, y_test, seed_value)\n",
    "                elapsed_time = time.time() - start_time #in seconds\n",
    "\n",
    "                if isinstance(features, str) and features == 'full':\n",
    "                    feature_names = csv_data.columns.tolist()\n",
    "                else:\n",
    "                    feature_names = [csv_data.columns[idx] for idx in features]\n",
    "\n",
    "                results_df = pd.concat([results_df, pd.DataFrame({\n",
    "                    'Approach': [function_name],\n",
    "                    'Accuracy': [metrics['Accuracy']],\n",
    "                    'Precision': [metrics['Precision']],\n",
    "                    'Recall': [metrics['Recall']],\n",
    "                    'F1 Score': [metrics['F1 Score']],\n",
    "                    'AUC': [metrics['AUC']],\n",
    "                    'G-mean': [metrics['G-mean']],\n",
    "                    'Matthew': [metrics['Matthew']],\n",
    "                    'Cohen': [metrics['Cohen']],\n",
    "                    'Feature No': [metrics['Feature No']],\n",
    "                    'Elapsed Time (sec)': [elapsed_time], #in seconds\n",
    "                    'qpu_access_time': [get_info(info, 'qpu_access_time')],\n",
    "                    'charge_time': [get_info(info, 'charge_time')],\n",
    "                    'run_time': [get_info(info, 'run_time')],\n",
    "                    'qpu_sampling_time': [get_info(info, 'qpu_sampling_time')],\n",
    "                    'qpu_anneal_time_per_sample': [get_info(info, 'qpu_anneal_time_per_sample')],\n",
    "                    'qpu_readout_time_per_sample': [get_info(info, 'qpu_readout_time_per_sample')],\n",
    "                    'qpu_access_overhead_time': [get_info(info, 'qpu_access_overhead_time')],\n",
    "                    'qpu_programming_time': [get_info(info, 'qpu_programming_time')],\n",
    "                    'qpu_delay_time_per_sample': [get_info(info, 'qpu_delay_time_per_sample')],\n",
    "                    'total_post_processing_time': [get_info(info, 'total_post_processing_time')],\n",
    "                    'post_processing_overhead_time': [get_info(info, 'post_processing_overhead_time')],\n",
    "                    'problem_label': [get_info(info, 'problem_label')],\n",
    "                    'problem_id': [get_info(info, 'problem_id')],\n",
    "                    'Features': [', '.join(feature_names)],\n",
    "                    'Time Unit': ['Micro Sec']\n",
    "                })], ignore_index=True)\n",
    "\n",
    "        if os.path.exists(result_file_path):\n",
    "            with pd.ExcelWriter(result_file_path, mode=\"a\", engine=\"openpyxl\", if_sheet_exists=\"replace\") as writer:\n",
    "                results_df.to_excel(writer, sheet_name=file_name.split('.')[0], index=False)\n",
    "        else:\n",
    "            with pd.ExcelWriter(result_file_path, engine=\"xlsxwriter\") as writer:\n",
    "                results_df.to_excel(writer, sheet_name=file_name.split('.')[0], index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e32f57-7360-4f09-99ba-c5e61209a9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing EQ.csv with 39 features...\n",
      "Accuracy: 0.8076923076923077\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.74      0.79        39\n",
      "           1       0.77      0.87      0.82        39\n",
      "\n",
      "    accuracy                           0.81        78\n",
      "   macro avg       0.81      0.81      0.81        78\n",
      "weighted avg       0.81      0.81      0.81        78\n",
      "\n",
      "Processing JDT.csv with 43 features...\n",
      "Accuracy: 0.8107255520504731\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       164\n",
      "           1       0.83      0.77      0.80       153\n",
      "\n",
      "    accuracy                           0.81       317\n",
      "   macro avg       0.81      0.81      0.81       317\n",
      "weighted avg       0.81      0.81      0.81       317\n",
      "\n",
      "Processing Lucene.csv with 40 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\venv_dw\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [16 43 45] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\User\\venv_dw\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.848605577689243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86       142\n",
      "           1       0.81      0.84      0.83       109\n",
      "\n",
      "    accuracy                           0.85       251\n",
      "   macro avg       0.85      0.85      0.85       251\n",
      "weighted avg       0.85      0.85      0.85       251\n",
      "\n",
      "Processing Mylyn.csv with 38 features...\n",
      "Accuracy: 0.7758887171561051\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.81      0.79       329\n",
      "           1       0.79      0.74      0.76       318\n",
      "\n",
      "    accuracy                           0.78       647\n",
      "   macro avg       0.78      0.78      0.78       647\n",
      "weighted avg       0.78      0.78      0.78       647\n",
      "\n",
      "Processing PDE.csv with 39 features...\n",
      "Accuracy: 0.7732558139534884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.81      0.78       258\n",
      "           1       0.79      0.74      0.76       258\n",
      "\n",
      "    accuracy                           0.77       516\n",
      "   macro avg       0.77      0.77      0.77       516\n",
      "weighted avg       0.77      0.77      0.77       516\n",
      "\n",
      "Processing activemq.csv with 44 features...\n",
      "Accuracy: 0.8618524332810047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87       321\n",
      "           1       0.89      0.82      0.86       316\n",
      "\n",
      "    accuracy                           0.86       637\n",
      "   macro avg       0.86      0.86      0.86       637\n",
      "weighted avg       0.86      0.86      0.86       637\n",
      "\n",
      "Processing groovy.csv with 44 features...\n",
      "Accuracy: 0.840531561461794\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.83       149\n",
      "           1       0.80      0.91      0.85       152\n",
      "\n",
      "    accuracy                           0.84       301\n",
      "   macro avg       0.85      0.84      0.84       301\n",
      "weighted avg       0.85      0.84      0.84       301\n",
      "\n",
      "Processing hbase.csv with 43 features...\n",
      "Accuracy: 0.7744807121661721\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.77       178\n",
      "           1       0.73      0.82      0.78       159\n",
      "\n",
      "    accuracy                           0.77       337\n",
      "   macro avg       0.78      0.78      0.77       337\n",
      "weighted avg       0.78      0.77      0.77       337\n",
      "\n",
      "Processing hive.csv with 42 features...\n",
      "Accuracy: 0.788546255506608\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.82       246\n",
      "           1       0.81      0.70      0.75       208\n",
      "\n",
      "    accuracy                           0.79       454\n",
      "   macro avg       0.79      0.78      0.78       454\n",
      "weighted avg       0.79      0.79      0.79       454\n",
      "\n",
      "Processing jruby.csv with 46 features...\n",
      "Accuracy: 0.8643410852713178\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       140\n",
      "           1       0.85      0.85      0.85       118\n",
      "\n",
      "    accuracy                           0.86       258\n",
      "   macro avg       0.86      0.86      0.86       258\n",
      "weighted avg       0.86      0.86      0.86       258\n",
      "\n",
      "Processing wicket.csv with 45 features...\n",
      "Accuracy: 0.8195718654434251\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83       348\n",
      "           1       0.81      0.81      0.81       306\n",
      "\n",
      "    accuracy                           0.82       654\n",
      "   macro avg       0.82      0.82      0.82       654\n",
      "weighted avg       0.82      0.82      0.82       654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To compare with simple selectKBest algorithm by selecting same number of best features we find from QA process above. \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Dataset names and corresponding number of features to select\n",
    "dataset_feature_map = {\n",
    "    'EQ.csv': 39,\n",
    "    'JDT.csv': 43,\n",
    "    'Lucene.csv': 40,\n",
    "    'Mylyn.csv': 38,\n",
    "    'PDE.csv': 39,\n",
    "    'activemq.csv': 44,\n",
    "    'groovy.csv': 44,\n",
    "    'hbase.csv': 43,\n",
    "    'hive.csv': 42,\n",
    "    'jruby.csv': 46,\n",
    "    'wicket.csv': 45\n",
    "}\n",
    "\n",
    "# Set random seed\n",
    "seed_value = 42\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\" Load CSV data \"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def feature_selection(X, y, k):\n",
    "    \"\"\" Perform feature selection with SelectKBest \"\"\"\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    return X_new, selector\n",
    "\n",
    "def defect_prediction(X_train, X_test, y_train, y_test):\n",
    "    \"\"\" Train a SVC classifier and predict defects \"\"\"\n",
    "    classifier = SVC(random_state=seed_value)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    # Print performance metrics\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "def process_dataset(file, k):\n",
    "    \"\"\" Process a single dataset file with k selected features \"\"\"\n",
    "    # Load dataset\n",
    "    data = load_data(file)\n",
    "    \n",
    "    # Split features and labels\n",
    "    X = data.iloc[:, :-1]  # All columns except the last one\n",
    "    y = data.iloc[:, -1]   # Last column is the label\n",
    "\n",
    "    # Impute missing values with the mean\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X = imputer.fit_transform(X)\n",
    "\n",
    "    X = X.astype(float)\n",
    "    y = y.astype(int)\n",
    "\n",
    "    # Handle imbalanced data using SMOTE\n",
    "    smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=seed_value)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Split the resampled data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=seed_value)\n",
    "\n",
    "    # Normalize the features\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Perform feature selection with k features\n",
    "    X_train_fs, selector = feature_selection(X_train_scaled, y_train, k=k)\n",
    "    X_test_fs = selector.transform(X_test_scaled)\n",
    "    \n",
    "    # Perform defect prediction\n",
    "    defect_prediction(X_train_fs, X_test_fs, y_train, y_test)\n",
    "\n",
    "# Process each dataset with corresponding number of features\n",
    "for dataset, k in dataset_feature_map.items():\n",
    "    print(f\"Processing {dataset} with {k} features...\")\n",
    "    process_dataset(dataset, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5e9faf8-51b2-469f-a1ad-f371d03e8c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'constraint_labels': [], 'qpu_access_time': 15987, 'charge_time': 4306878, 'run_time': 4306878, 'problem_id': '7a116022-e988-47e3-9129-193e6111a58c', 'problem_label': 'DW-TEST_MC2'}\n",
    "\n",
    "d2 = {'timing': {'qpu_sampling_time': 12732.0, 'qpu_anneal_time_per_sample': 20.0, 'qpu_readout_time_per_sample': 86.74, 'qpu_access_time': 28495.16, 'qpu_access_overhead_time': 662.84, 'qpu_programming_time': 15763.16, 'qpu_delay_time_per_sample': 20.58, 'total_post_processing_time': 23.0, 'post_processing_overhead_time': 23.0}, 'problem_id': '7d1388ee-1bbd-40f0-9a5b-e24a275ad0ea', 'problem_label': 'QPU-TEST_MC2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7f6a7c89-bf54-4dc4-96e8-1229a271fca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraint_labels []\n",
      "qpu_access_time 15987\n",
      "charge_time 4306878\n",
      "run_time 4306878\n",
      "problem_id 7a116022-e988-47e3-9129-193e6111a58c\n",
      "problem_label DW-TEST_MC2\n",
      "\n",
      "qpu_sampling_time 12732.0\n",
      "qpu_anneal_time_per_sample 20.0\n",
      "qpu_readout_time_per_sample 86.74\n",
      "qpu_access_time 28495.16\n",
      "qpu_access_overhead_time 662.84\n",
      "qpu_programming_time 15763.16\n",
      "qpu_delay_time_per_sample 20.58\n",
      "total_post_processing_time 23.0\n",
      "post_processing_overhead_time 23.0\n",
      "problem_id 7d1388ee-1bbd-40f0-9a5b-e24a275ad0ea\n",
      "problem_label QPU-TEST_MC2\n"
     ]
    }
   ],
   "source": [
    "def show_dict(var_dict):\n",
    "    for key in var_dict: \n",
    "        if isinstance(var_dict[key], dict):\n",
    "            show_dict(var_dict[key])\n",
    "        else: \n",
    "            print(key, var_dict[key])\n",
    "\n",
    "show_dict(d1)  \n",
    "print()\n",
    "show_dict(d2)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5409cd56-fbbd-428a-a4c5-9784dff7155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "662.84"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getinfo2(d2, 'qpu_access_overhead_time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dwExpEnv",
   "language": "python",
   "name": "venv_dw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
